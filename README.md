# 📊 Zrive-ds: Applied Data Science

This repository contains my work from the **Zrive DS (Applied Data Science)** program, where I developed hands-on projects simulating a **real-world work environment**.

The goal was to build **end-to-end experience in Data Science and Backend development**, following **clean coding practices** — from data acquisition to deploying a functional API consuming Machine Learning models.

---

## 📂 Project Structure

The repository is organized into **6 modules**, each focusing on a key step in the Data Science workflow.

### 🔹 **Module 1 – Data Collection & Visualization**

* Collected data from a **public weather API**.
* Built **dynamic dashboards** showing historical climate data.
* Processed and cleaned datasets in **JSON and CSV** formats.

### 🔹 **Module 2 – Exploratory Data Analysis (EDA)**

* Performed **EDA on a grocery store dataset**.
* Cleaned, transformed, and analyzed the data.
* Discovered patterns and created **visual insights** using **Pandas, Matplotlib, and Seaborn**.

### 🔹 **Module 3 – Feature Engineering & Linear Models**

* Designed and engineered **new features**.
* Trained **linear models** including **Linear Regression and Logistic Regression (with L1 and L2 regularization)**.
* Evaluated performance with multiple **classification and regression metrics**.

### 🔹 **Module 4 – Advanced Machine Learning Models**

* Implemented **non-linear and advanced ML models**.
* Built from **Random Forests to more complex algorithms**.
* Compared performance, applied hyperparameter tuning, and cross-validation.

### 🔹 **Module 5 – Data Leakage Analysis**

* Conducted a critical review of a previous project to detect potential **data leakage issues**.
* Documented findings and proposed solutions to prevent future leakage in ML pipelines.

### 🔹 **Module 6 – API Deployment with ML Models**

* Designed and deployed a **local API using FastAPI**.
* Implemented the full workflow:

  1. Receiving user requests.
  2. Querying a **PostgreSQL database**.
  3. Data transformation and preparation.
  4. Model training and prediction.
  5. Returning predictions as API responses.
* Built with **clean architecture principles and best coding practices**.

---

## 🛠️ Tech Stack

* **Languages:** Python, SQL
* **Libraries:** Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, FastAPI
* **Databases:** PostgreSQL, SQLite
* **Tools:** Git, Jupyter, Docker (concepts)

---

## 🚀 Key Learnings

* Built a **complete end-to-end Data Science workflow**.
* Designed **ETL pipelines** and processed real-world datasets.
* Trained, validated, and compared **ML models from basic to advanced**.
* Identified and mitigated risks like **data leakage**.
* Created a **functional API** integrating databases and ML models.
* Applied **software engineering best practices** with Git/GitHub version control.

---

## 📌 Note

This repository showcases a **practical training program designed to simulate real industry experience** in the fields of **Data Science and Backend development**.

---
